{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ea20e0-59a1-40d4-9952-b323d2b0c560",
   "metadata": {},
   "source": [
    "# Projects (Pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6fe109-2462-445f-b475-d59a02c8e689",
   "metadata": {},
   "source": [
    "'''\n",
    "Project 1-IoT Sensor Logs Analysis\n",
    "\n",
    "Domain: IoT\n",
    "\n",
    "Goal: \n",
    "  To analyze real-time IoT sensor logs to ensure sensor data reliability, detect anomalies, and generate actionable insights like sensor performance trends, error patterns, and environmental conditions over time.\n",
    "\n",
    "Objectives:\n",
    "  Fill missing values\n",
    "  Count ALERTs per sensor\n",
    "  Get average temperature per sensor\n",
    "  Detect outliers\n",
    "  Resample for 10-minute interval average\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b1d030d-b270-4643-8b05-0a331a572b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoT sensor data: \n",
      "             Timestamp Sensor_ID  Temperature  Humidity Status\n",
      "0 2025-05-18 08:00:00        S1         36.5      55.0     OK\n",
      "1 2025-05-18 08:05:00        S2         80.2      48.0  ALERT\n",
      "2 2025-05-18 08:10:00        S1         37.0      53.0     OK\n",
      "3 2025-05-18 08:15:00        S3         76.4      47.0  ALERT\n",
      "4 2025-05-18 08:20:00        S2         85.1      50.0  ALERT\n",
      "5 2025-05-18 08:25:00        S1         35.8       NaN     OK\n",
      "6 2025-05-18 08:30:00        S3         78.5      46.0  ALERT\n",
      "7 2025-05-18 08:35:00        S3         40.2      44.0     OK\n",
      "8 2025-05-18 08:40:00        S2         82.0      49.0  ALERT\n",
      "9 2025-05-18 08:45:00        S1          NaN      54.0     OK\n",
      "\n",
      "✅ Step 1 - After Filling Missing Values:\n",
      "             Timestamp Sensor_ID  Temperature   Humidity Status\n",
      "0 2025-05-18 08:00:00        S1         36.5  55.000000     OK\n",
      "1 2025-05-18 08:05:00        S2         80.2  48.000000  ALERT\n",
      "2 2025-05-18 08:10:00        S1         37.0  53.000000     OK\n",
      "3 2025-05-18 08:15:00        S3         76.4  47.000000  ALERT\n",
      "4 2025-05-18 08:20:00        S2         85.1  50.000000  ALERT\n",
      "5 2025-05-18 08:25:00        S1         35.8  49.555556     OK\n",
      "6 2025-05-18 08:30:00        S3         78.5  46.000000  ALERT\n",
      "7 2025-05-18 08:35:00        S3         40.2  44.000000     OK\n",
      "8 2025-05-18 08:40:00        S2         82.0  49.000000  ALERT\n",
      "9 2025-05-18 08:45:00        S1         61.3  54.000000     OK\n",
      "\n",
      "✅ Step 2 - ALERT Counts per Sensor:\n",
      " Sensor_ID\n",
      "S2    3\n",
      "S3    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ Step 3 - Average Temperature per Sensor:\n",
      " Sensor_ID\n",
      "S1    42.650000\n",
      "S2    82.433333\n",
      "S3    65.033333\n",
      "Name: Temperature, dtype: float64\n",
      "\n",
      "✅ Step 4 - Temperature Outliers (>80°C):\n",
      "             Timestamp Sensor_ID  Temperature\n",
      "1 2025-05-18 08:05:00        S2         80.2\n",
      "4 2025-05-18 08:20:00        S2         85.1\n",
      "8 2025-05-18 08:40:00        S2         82.0\n",
      "\n",
      "✅ Step 5 - 10-Minute Interval Average:\n",
      "                      Temperature   Humidity\n",
      "Timestamp                                  \n",
      "2025-05-18 08:00:00        58.35  51.500000\n",
      "2025-05-18 08:10:00        56.70  50.000000\n",
      "2025-05-18 08:20:00        60.45  49.777778\n",
      "2025-05-18 08:30:00        59.35  45.000000\n",
      "2025-05-18 08:40:00        71.65  51.500000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 0: Create sample IoT sensor data\n",
    "data = {\n",
    "    'Timestamp': [\n",
    "        '2025-05-18 08:00:00', '2025-05-18 08:05:00', '2025-05-18 08:10:00',\n",
    "        '2025-05-18 08:15:00', '2025-05-18 08:20:00', '2025-05-18 08:25:00',\n",
    "        '2025-05-18 08:30:00', '2025-05-18 08:35:00', '2025-05-18 08:40:00',\n",
    "        '2025-05-18 08:45:00'\n",
    "    ],\n",
    "    'Sensor_ID': ['S1', 'S2', 'S1', 'S3', 'S2', 'S1', 'S3', 'S3', 'S2', 'S1'],\n",
    "    'Temperature': [36.5, 80.2, 37.0, 76.4, 85.1, 35.8, 78.5, 40.2, 82.0, None],\n",
    "    'Humidity': [55.0, 48.0, 53.0, 47.0, 50.0, None, 46.0, 44.0, 49.0, 54.0],\n",
    "    'Status': ['OK', 'ALERT', 'OK', 'ALERT', 'ALERT', 'OK', 'ALERT', 'OK', 'ALERT', 'OK']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "print(\"IoT sensor data: \\n\", df)\n",
    "# Step 1: Fill missing values\n",
    "df['Temperature'] = df['Temperature'].fillna(df['Temperature'].mean())\n",
    "df['Humidity'] = df['Humidity'].fillna(df['Humidity'].mean())\n",
    "print(\"\\n✅ Step 1 - After Filling Missing Values:\\n\", df)\n",
    "\n",
    "# Step 2: Count ALERTs per Sensor\n",
    "alerts_per_sensor = df[df['Status'] == 'ALERT']['Sensor_ID'].value_counts()\n",
    "print(\"\\n✅ Step 2 - ALERT Counts per Sensor:\\n\", alerts_per_sensor)\n",
    "\n",
    "# Step 3: Average Temperature per Sensor\n",
    "avg_temp = df.groupby('Sensor_ID')['Temperature'].mean()\n",
    "print(\"\\n✅ Step 3 - Average Temperature per Sensor:\\n\", avg_temp)\n",
    "\n",
    "# Step 4: Detect Temperature Outliers (e.g., > 80°C)\n",
    "outliers = df[df['Temperature'] > 80]\n",
    "print(\"\\n✅ Step 4 - Temperature Outliers (>80°C):\\n\", outliers[['Timestamp', 'Sensor_ID', 'Temperature']])\n",
    "\n",
    "# Step 5: Resample to 10-minute average (across all sensors)\n",
    "df.set_index('Timestamp', inplace=True) #You need a DateTimeIndex for resampling.This line converts the 'Timestamp' column into the index, so you can resample based on time intervals.\n",
    "resampled_avg = df.resample('10min').mean(numeric_only=True) \n",
    "#df.resample('10min')-->This groups the data into 10-minute intervals.For example: 00:00–00:10, 00:10–00:20, etc.\n",
    "#.mean(numeric_only=True) -->It calculates the average for all numeric columns within each 10-minute window.numeric_only=True ensures non-numeric columns (like strings) are ignored.\n",
    "print(\"\\n✅ Step 5 - 10-Minute Interval Average:\\n\", resampled_avg)\n",
    "#So, the temperature and humidity values change because they are the average of all sensors’ readings in that 10-minute window, not just one sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413861e8-6987-426b-9ced-5a87341fa1d2",
   "metadata": {},
   "source": [
    "#Project 2: Cloud IoT Device Management \n",
    "'''\n",
    "Domain:\n",
    "  Cloud IoT Device Management\n",
    "\n",
    "Goal:\n",
    "  To analyze and monitor IoT devices’ status and data usage patterns from cloud logs, helping manage device activity and optimize performance.\n",
    "\n",
    "Objectives:\n",
    "  Identify how many devices are active vs inactive.\n",
    "  Track data usage per device.\n",
    "  Summarize average data usage by device type.\n",
    "  Detect any missing data in device reports and handle it.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a400a77b-1364-4c80-b35e-211775065000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   DeviceID DeviceType    Status  DataUsageMB\n",
      "0       D1     Sensor    Active        120.0\n",
      "1       D2   Actuator  Inactive          NaN\n",
      "2       D3     Sensor    Active        350.0\n",
      "3       D4     Camera    Active        400.0\n",
      "4       D5     Camera  Inactive        150.0\n",
      "5       D6   Actuator    Active          NaN\n",
      "\n",
      "Step 1 - Device Status Counts:\n",
      " Status\n",
      "Active      4\n",
      "Inactive    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Step 2 - Data with Missing Values Filled:\n",
      "   DeviceID DeviceType    Status  DataUsageMB\n",
      "0       D1     Sensor    Active        120.0\n",
      "1       D2   Actuator  Inactive        255.0\n",
      "2       D3     Sensor    Active        350.0\n",
      "3       D4     Camera    Active        400.0\n",
      "4       D5     Camera  Inactive        150.0\n",
      "5       D6   Actuator    Active        255.0\n",
      "\n",
      "Step 3 - Average Data Usage by Device Type:\n",
      " DeviceType\n",
      "Actuator    255.0\n",
      "Camera      275.0\n",
      "Sensor      235.0\n",
      "Name: DataUsageMB, dtype: float64\n",
      "\n",
      "Step 4 - Active Devices:\n",
      "   DeviceID DeviceType  Status  DataUsageMB\n",
      "0       D1     Sensor  Active        120.0\n",
      "2       D3     Sensor  Active        350.0\n",
      "3       D4     Camera  Active        400.0\n",
      "5       D6   Actuator  Active        255.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample cloud IoT device logs data\n",
    "data = {\n",
    "    'DeviceID': ['D1', 'D2', 'D3', 'D4', 'D5', 'D6'],\n",
    "    'DeviceType': ['Sensor', 'Actuator', 'Sensor', 'Camera', 'Camera', 'Actuator'],\n",
    "    'Status': ['Active', 'Inactive', 'Active', 'Active', 'Inactive', 'Active'],\n",
    "    'DataUsageMB': [120, np.nan, 350, 400, 150, np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Data:\\n\", df)\n",
    "\n",
    "# Step 1: Count devices by status\n",
    "device_status_counts = df['Status'].value_counts()\n",
    "print(\"\\nStep 1 - Device Status Counts:\\n\", device_status_counts)\n",
    "\n",
    "# Step 2: Fill missing DataUsageMB with the mean data usage\n",
    "mean_data_usage = df['DataUsageMB'].mean()\n",
    "df['DataUsageMB'] = df['DataUsageMB'].fillna(mean_data_usage)\n",
    "print(\"\\nStep 2 - Data with Missing Values Filled:\\n\", df)\n",
    "\n",
    "# Step 3: Average data usage by DeviceType\n",
    "avg_data_usage_by_type = df.groupby('DeviceType')['DataUsageMB'].mean()\n",
    "print(\"\\nStep 3 - Average Data Usage by Device Type:\\n\", avg_data_usage_by_type)\n",
    "\n",
    "# Step 4: Filter Active devices only\n",
    "active_devices = df[df['Status'] == 'Active']\n",
    "print(\"\\nStep 4 - Active Devices:\\n\", active_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd160dc-824e-4445-8ede-237fbad74e2c",
   "metadata": {},
   "source": [
    "'''\n",
    "Project 3: Student Performance Dashboard\n",
    "\n",
    "Domain:\n",
    "  Education / Academic Analytics\n",
    "\n",
    "Goal:\n",
    "  To analyze student performance data to gain insights into subject-wise scores, attendance, and overall academic performance for decision-making and personalized interventions.\n",
    "\n",
    "Objectives:\n",
    "  Identify average marks per subject.\n",
    "  Categorize students based on performance.\n",
    "  Handle missing data in scores.\n",
    "  Analyze attendance and its correlation with performance.\n",
    "  Display students with top and bottom scores.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1b059fb-3a82-4dda-bb0a-8afd0ba27217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 - Original Student Data:\n",
      "   StudentID  Maths  Science  English  Attendance\n",
      "0        S1   88.0     85.0     90.0          92\n",
      "1        S2   92.0      NaN     87.0          88\n",
      "2        S3    NaN     78.0     85.0          75\n",
      "3        S4   70.0     65.0     72.0          80\n",
      "4        S5   60.0     55.0      NaN          60\n",
      "\n",
      "Step 1 - After Filling Missing Values:\n",
      "   StudentID  Maths  Science  English  Attendance\n",
      "0        S1   88.0    85.00     90.0          92\n",
      "1        S2   92.0    70.75     87.0          88\n",
      "2        S3   77.5    78.00     85.0          75\n",
      "3        S4   70.0    65.00     72.0          80\n",
      "4        S5   60.0    55.00     83.5          60\n",
      "\n",
      "Step 2 - Student Average Scores:\n",
      "   StudentID  AverageScore\n",
      "0        S1     87.666667\n",
      "1        S2     83.250000\n",
      "2        S3     80.166667\n",
      "3        S4     69.000000\n",
      "4        S5     66.166667\n",
      "\n",
      "Step 3 - Categorized Performance:\n",
      "   StudentID PerformanceCategory\n",
      "0        S1                Good\n",
      "1        S2                Good\n",
      "2        S3                Good\n",
      "3        S4             Average\n",
      "4        S5             Average\n",
      "\n",
      "Step 4 - Correlation between Attendance and Average Score: 0.8178476175940805\n",
      "\n",
      "Step 5 - Top Performers:\n",
      "   StudentID  AverageScore\n",
      "0        S1     87.666667\n",
      "1        S2     83.250000\n",
      "\n",
      "Step 5 - Bottom Performers:\n",
      "   StudentID  AverageScore\n",
      "4        S5     66.166667\n",
      "3        S4     69.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 0: Create a sample DataFrame for student performance\n",
    "data = {\n",
    "    'StudentID': ['S1', 'S2', 'S3', 'S4', 'S5'],\n",
    "    'Maths': [88, 92, np.nan, 70, 60],\n",
    "    'Science': [85, np.nan, 78, 65, 55],\n",
    "    'English': [90, 87, 85, 72, np.nan],\n",
    "    'Attendance': [92, 88, 75, 80, 60]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Step 0 - Original Student Data:\\n\", df)\n",
    "\n",
    "# Step 1: Handle missing scores by replacing with subject average\n",
    "df['Maths'] = df['Maths'].fillna(df['Maths'].mean())\n",
    "df['Science'] = df['Science'].fillna(df['Science'].mean())\n",
    "df['English'] = df['English'].fillna(df['English'].mean())\n",
    "print(\"\\nStep 1 - After Filling Missing Values:\\n\", df)\n",
    "\n",
    "# Step 2: Calculate average score for each student\n",
    "df['AverageScore'] = df[['Maths', 'Science', 'English']].mean(axis=1)\n",
    "print(\"\\nStep 2 - Student Average Scores:\\n\", df[['StudentID', 'AverageScore']])\n",
    "\n",
    "# Step 3: Categorize performance\n",
    "df['PerformanceCategory'] = pd.cut(df['AverageScore'],\n",
    "                                   bins=[0, 60, 75, 90, 100],\n",
    "                                   labels=['Poor', 'Average', 'Good', 'Excellent'])\n",
    "print(\"\\nStep 3 - Categorized Performance:\\n\", df[['StudentID', 'PerformanceCategory']])\n",
    "\n",
    "# Step 4: Correlation between attendance and performance\n",
    "correlation = df['Attendance'].corr(df['AverageScore'])\n",
    "print(\"\\nStep 4 - Correlation between Attendance and Average Score:\", correlation)\n",
    "\n",
    "# Step 5: Display top and bottom performers\n",
    "top_students = df.sort_values(by='AverageScore', ascending=False).head(2)\n",
    "bottom_students = df.sort_values(by='AverageScore').head(2)\n",
    "print(\"\\nStep 5 - Top Performers:\\n\", top_students[['StudentID', 'AverageScore']])\n",
    "print(\"\\nStep 5 - Bottom Performers:\\n\", bottom_students[['StudentID', 'AverageScore']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4e3b2-e940-46b4-8f8f-e3d55fa6acde",
   "metadata": {},
   "source": [
    "'''\n",
    "Project 4: Music Listening Pattern Analysis\n",
    "\n",
    "Domain:\n",
    "  Entertainment / User Behavior Analytics\n",
    "\n",
    "Goal:\n",
    "  To analyze users' music listening habits and discover patterns such as popular genres, listening durations, and user engagement.\n",
    "\n",
    "Objectives:\n",
    "  Analyze most frequently played genres.\n",
    "  Calculate average listening time per user.\n",
    "  Detect inactive users.\n",
    "  Visualize daily listening trends.\n",
    "  Handle missing data in play durations.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94829b65-25e2-4d68-ac7d-57beef0bd57b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 - Original Music Listening Data:\n",
      "   UserID Genre  PlayDuration       Date\n",
      "0     U1   Pop           3.5 2025-05-01\n",
      "1     U2  Rock           4.0 2025-05-02\n",
      "2     U3  Jazz           NaN 2025-05-03\n",
      "3     U4   Pop           3.0 2025-05-04\n",
      "4     U5  Rock           2.5 2025-05-05\n",
      "5     U1  Jazz           4.2 2025-05-06\n",
      "6     U2   Pop           3.8 2025-05-07\n",
      "7     U3  Rock           2.9 2025-05-08\n",
      "8     U4  Jazz           NaN 2025-05-09\n",
      "9     U5   Pop           3.6 2025-05-10\n",
      "\n",
      "Step 1 - After Handling Missing PlayDuration:\n",
      "   UserID Genre  PlayDuration       Date\n",
      "0     U1   Pop        3.5000 2025-05-01\n",
      "1     U2  Rock        4.0000 2025-05-02\n",
      "2     U3  Jazz        3.4375 2025-05-03\n",
      "3     U4   Pop        3.0000 2025-05-04\n",
      "4     U5  Rock        2.5000 2025-05-05\n",
      "5     U1  Jazz        4.2000 2025-05-06\n",
      "6     U2   Pop        3.8000 2025-05-07\n",
      "7     U3  Rock        2.9000 2025-05-08\n",
      "8     U4  Jazz        3.4375 2025-05-09\n",
      "9     U5   Pop        3.6000 2025-05-10\n",
      "\n",
      "Step 2 - Genre Frequency:\n",
      " Genre\n",
      "Pop     4\n",
      "Rock    3\n",
      "Jazz    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Step 3 - Average Listening Time per User:\n",
      " UserID\n",
      "U1    3.85000\n",
      "U2    3.90000\n",
      "U3    3.16875\n",
      "U4    3.21875\n",
      "U5    3.05000\n",
      "Name: PlayDuration, dtype: float64\n",
      "\n",
      "Step 4 - Inactive Users (Low Avg Listening):\n",
      " Series([], Name: PlayDuration, dtype: float64)\n",
      "\n",
      "Step 5 - Daily Listening Trends:\n",
      " Date\n",
      "2025-05-01    3.5000\n",
      "2025-05-02    4.0000\n",
      "2025-05-03    3.4375\n",
      "2025-05-04    3.0000\n",
      "2025-05-05    2.5000\n",
      "2025-05-06    4.2000\n",
      "2025-05-07    3.8000\n",
      "2025-05-08    2.9000\n",
      "2025-05-09    3.4375\n",
      "2025-05-10    3.6000\n",
      "Name: PlayDuration, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 0: Create sample music listening data\n",
    "data = {\n",
    "    'UserID': ['U1', 'U2', 'U3', 'U4', 'U5', 'U1', 'U2', 'U3', 'U4', 'U5'],\n",
    "    'Genre': ['Pop', 'Rock', 'Jazz', 'Pop', 'Rock', 'Jazz', 'Pop', 'Rock', 'Jazz', 'Pop'],\n",
    "    'PlayDuration': [3.5, 4.0, np.nan, 3.0, 2.5, 4.2, 3.8, 2.9, np.nan, 3.6],\n",
    "    'Date': pd.date_range(start='2025-05-01', periods=10, freq='D')\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Step 0 - Original Music Listening Data:\\n\", df)\n",
    "\n",
    "# Step 1: Fill missing PlayDuration with mean\n",
    "df['PlayDuration'] = df['PlayDuration'].fillna(df['PlayDuration'].mean())\n",
    "print(\"\\nStep 1 - After Handling Missing PlayDuration:\\n\", df)\n",
    "\n",
    "# Step 2: Most frequently played genre\n",
    "genre_counts = df['Genre'].value_counts()\n",
    "print(\"\\nStep 2 - Genre Frequency:\\n\", genre_counts)\n",
    "\n",
    "# Step 3: Average listening time per user\n",
    "avg_listen_per_user = df.groupby('UserID')['PlayDuration'].mean()\n",
    "print(\"\\nStep 3 - Average Listening Time per User:\\n\", avg_listen_per_user)\n",
    "\n",
    "# Step 4: Identify inactive users (below 3.0 minutes average)\n",
    "inactive_users = avg_listen_per_user[avg_listen_per_user < 3.0]\n",
    "print(\"\\nStep 4 - Inactive Users (Low Avg Listening):\\n\", inactive_users)\n",
    "\n",
    "# Step 5: Daily listening trends (total play duration per day)\n",
    "daily_trend = df.groupby('Date')['PlayDuration'].sum()\n",
    "print(\"\\nStep 5 - Daily Listening Trends:\\n\", daily_trend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a419398a-bc72-48f7-908b-a8fa3f07db76",
   "metadata": {},
   "source": [
    "'''\n",
    "Project 5: VLSI Chip Test Report Analysis with everything structured properly.\n",
    "\n",
    "Domain:\n",
    " VLSI (Very-Large-Scale Integration) Testing and Quality Analysis\n",
    "\n",
    "Goal:\n",
    " To analyze chip testing results to identify failure patterns, quality distribution, and optimize the manufacturing process.\n",
    "\n",
    "Objectives:\n",
    " Load and clean chip test data.\n",
    " Identify chips that failed testing.\n",
    " Analyze failure rates by chip type.\n",
    " Calculate average test scores.\n",
    " Visualize pass/fail trends over time.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8082dc3b-88d8-43e0-a36b-495b2fca6956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Step 0 - Raw VLSI Chip Test Data:\n",
      "   ChipID ChipType  TestScore   TestDate\n",
      "0   C001     ASIC       92.0 2025-04-01\n",
      "1   C002     FPGA       85.0 2025-04-02\n",
      "2   C003     ASIC       78.0 2025-04-03\n",
      "3   C004     ASIC       65.0 2025-04-04\n",
      "4   C005     FPGA       50.0 2025-04-05\n",
      "5   C006     FPGA       89.0 2025-04-06\n",
      "6   C007     ASIC        NaN 2025-04-07\n",
      "7   C008     FPGA       70.0 2025-04-08\n",
      "8   C009     ASIC       95.0 2025-04-09\n",
      "9   C010     FPGA       40.0 2025-04-10\n",
      "\n",
      "Step 1 - After Filling Missing Test Scores:\n",
      "   ChipID ChipType  TestScore   TestDate\n",
      "0   C001     ASIC  92.000000 2025-04-01\n",
      "1   C002     FPGA  85.000000 2025-04-02\n",
      "2   C003     ASIC  78.000000 2025-04-03\n",
      "3   C004     ASIC  65.000000 2025-04-04\n",
      "4   C005     FPGA  50.000000 2025-04-05\n",
      "5   C006     FPGA  89.000000 2025-04-06\n",
      "6   C007     ASIC  73.777778 2025-04-07\n",
      "7   C008     FPGA  70.000000 2025-04-08\n",
      "8   C009     ASIC  95.000000 2025-04-09\n",
      "9   C010     FPGA  40.000000 2025-04-10\n",
      "\n",
      "Step 2 - Pass/Fail Marking:\n",
      "   ChipID  TestScore Result\n",
      "0   C001  92.000000   Pass\n",
      "1   C002  85.000000   Pass\n",
      "2   C003  78.000000   Pass\n",
      "3   C004  65.000000   Fail\n",
      "4   C005  50.000000   Fail\n",
      "5   C006  89.000000   Pass\n",
      "6   C007  73.777778   Pass\n",
      "7   C008  70.000000   Pass\n",
      "8   C009  95.000000   Pass\n",
      "9   C010  40.000000   Fail\n",
      "\n",
      "Step 3 - Failed Chips:\n",
      "   ChipID ChipType  TestScore   TestDate Result\n",
      "3   C004     ASIC       65.0 2025-04-04   Fail\n",
      "4   C005     FPGA       50.0 2025-04-05   Fail\n",
      "9   C010     FPGA       40.0 2025-04-10   Fail\n",
      "\n",
      "Step 4 - Failure Rate by Chip Type (%):\n",
      " Result    Fail  Pass\n",
      "ChipType            \n",
      "ASIC      20.0  80.0\n",
      "FPGA      40.0  60.0\n",
      "\n",
      "Step 5 - Average Test Score by Chip Type:\n",
      " ChipType\n",
      "ASIC    80.755556\n",
      "FPGA    66.800000\n",
      "Name: TestScore, dtype: float64\n",
      "\n",
      "Step 6 - Daily Pass/Fail Trends:\n",
      " Result      Fail  Pass\n",
      "TestDate              \n",
      "2025-04-01   0.0   1.0\n",
      "2025-04-02   0.0   1.0\n",
      "2025-04-03   0.0   1.0\n",
      "2025-04-04   1.0   0.0\n",
      "2025-04-05   1.0   0.0\n",
      "2025-04-06   0.0   1.0\n",
      "2025-04-07   0.0   1.0\n",
      "2025-04-08   0.0   1.0\n",
      "2025-04-09   0.0   1.0\n",
      "2025-04-10   1.0   0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 0: Sample VLSI chip test report data\n",
    "data = {\n",
    "    'ChipID': ['C001', 'C002', 'C003', 'C004', 'C005', 'C006', 'C007', 'C008', 'C009', 'C010'],\n",
    "    'ChipType': ['ASIC', 'FPGA', 'ASIC', 'ASIC', 'FPGA', 'FPGA', 'ASIC', 'FPGA', 'ASIC', 'FPGA'],\n",
    "    'TestScore': [92, 85, 78, 65, 50, 89, np.nan, 70, 95, 40],\n",
    "    'TestDate': pd.date_range(start='2025-04-01', periods=10, freq='D')\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"📋 Step 0 - Raw VLSI Chip Test Data:\\n\", df)\n",
    "\n",
    "# Step 1: Fill missing TestScore with mean\n",
    "df['TestScore'] = df['TestScore'].fillna(df['TestScore'].mean())\n",
    "print(\"\\nStep 1 - After Filling Missing Test Scores:\\n\", df)\n",
    "\n",
    "# Step 2: Mark pass/fail based on score >= 70\n",
    "df['Result'] = df['TestScore'].apply(lambda x: 'Pass' if x >= 70 else 'Fail')\n",
    "print(\"\\nStep 2 - Pass/Fail Marking:\\n\", df[['ChipID', 'TestScore', 'Result']])\n",
    "\n",
    "# Step 3: Count failures\n",
    "failures = df[df['Result'] == 'Fail']\n",
    "print(\"\\nStep 3 - Failed Chips:\\n\", failures)\n",
    "\n",
    "# Step 4: Failure rate by chip type\n",
    "failure_rate = df.groupby('ChipType')['Result'].value_counts(normalize=True).unstack().fillna(0) * 100\n",
    "print(\"\\nStep 4 - Failure Rate by Chip Type (%):\\n\", failure_rate)\n",
    "\n",
    "# Step 5: Average Test Score by Chip Type\n",
    "avg_scores = df.groupby('ChipType')['TestScore'].mean()\n",
    "print(\"\\nStep 5 - Average Test Score by Chip Type:\\n\", avg_scores)\n",
    "\n",
    "# Step 6: Daily Test Summary\n",
    "daily_summary = df.groupby('TestDate')['Result'].value_counts().unstack().fillna(0)\n",
    "print(\"\\nStep 6 - Daily Pass/Fail Trends:\\n\", daily_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977a85f-b43c-46bb-b311-95170e76d7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
